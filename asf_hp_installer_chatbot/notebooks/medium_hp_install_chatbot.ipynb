{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b25a8ae4-4b90-480c-aee2-13331565d560",
   "metadata": {},
   "source": [
    "We know how powerful retrieval augmentation and conversational agents (chatbots) can be. Now with the power of Pinecone and Langchain we can combine them.\n",
    "Chatbots based with an LLM often struggle with data freshness, knowledge about specific domains, or accessing internal documentation. By coupling agents with retrieval augmentation tools we no longer have these problems.\n",
    "One the other side, using \"naive\" retrieval augmentation without the use of an LLM means we will retrieve contexts with every query. Again, this isn't always ideal as not every query requires access to external knowledge.\n",
    "\n",
    "Merging these methods gives us the best of both worlds. In this notebook we will attempt to create a chatbot which has a specialised knowledge base of heat pump installation guides. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cf2251-cd5b-4846-a9bf-f0ffc04a1a1a",
   "metadata": {},
   "source": [
    "To begin, we must install the prerequisite libraries that we will be using in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3350940b-8571-4285-a821-03c5124335d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "  pandas \\\n",
    "  langchain==0.1.0 \\\n",
    "  openai==1.7.1 \\\n",
    "  tiktoken==0.5.2 \\\n",
    "  \"pinecone-client[grpc]\"==2.2.1 \\\n",
    "  pinecone-datasets=='0.5.0rc11' \\\n",
    "  PyMuPDF \\\n",
    "  nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a353d0-b864-42e7-929d-9add7cdcefc8",
   "metadata": {},
   "source": [
    "Building a knowledge base by extracting the text from the PDF manual and then chunking the text up for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8209fe54-e18b-4728-bd08-aff00a98bc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to extract and chunk text from PDF files in a specified directory\n",
    "# To run this code, you'll need the NLTK and PyMuPDF (fitz) libraries. \n",
    "# Install them using: pip install nltk PyMuPDF\n",
    "import nltk\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "\n",
    "pdf_dir = '/Users/user_name/sustainability/installer_chatbot/air2water_HP_installation_guides/'\n",
    "texts = [] # List to hold text\n",
    "metadata_tags = []  # List to hold metadata tags\n",
    "for pdf_file in os.listdir(pdf_dir):\n",
    "    if pdf_file.endswith('.pdf'):\n",
    "        pdf_path = os.path.join(pdf_dir, pdf_file)\n",
    "        with fitz.open(pdf_path) as pdf_document:\n",
    "            pdf_text = ''\n",
    "            for page in pdf_document:\n",
    "                pdf_text += page.get_text()\n",
    "            texts.append(pdf_text)\n",
    "            metadata_tags.append(pdf_file) \n",
    "\n",
    "# ... Extract and chunk text from PDFs ...\n",
    "chunked_texts = []\n",
    "chunked_metadata_tags = []\n",
    "for text, metadata_tag in zip(texts, metadata_tags):\n",
    "    chunks = nltk.sent_tokenize(text) \n",
    "    chunk_tags = [metadata_tag] * len(chunks)\n",
    "    chunked_texts.extend(chunks)\n",
    "    chunked_metadata_tags.extend(chunk_tags)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e1843e-99fd-46c7-96d7-35a2ae46f048",
   "metadata": {},
   "source": [
    "Creating vector embeddings for text chunks from a PDF is like translating a story into a secret code that only computers can understand. Each sentence is turned into a series of numbers that captures its essence, allowing the computer to see how all the different sentences are related to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1de2479-be6a-4e34-ae23-588ce9bc4ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "\n",
    "client = OpenAI(api_key='<INSERT OPEN API KEY>') \n",
    "map_pdf_to_web_dictionary = {'Nibe_F2040_231844-5.pdf':'https://www.nibe.eu/assets/documents/16900/231844-5.pdf'} \n",
    "embeddings = []\n",
    "metadata_list = []  # This will contain our metadata with chunk and source\n",
    "document_counter = 1  # Starting with the first document\n",
    "chunk_counter = 0  # Initialize chunk counter\n",
    "prev_tag = None  # Keep track of the previous tag\n",
    "# Loop through the chunked texts and chunked meta tags created from the previous code snippet. \n",
    "for text, tag in zip(chunked_texts, chunked_metadata_tags):\n",
    "    response = client.embeddings.create(input=text,\n",
    "    model='text-embedding-ada-002')  # Use your chosen model ID)\n",
    "    embedding = response.data[0].embedding\n",
    "    embeddings.append(embedding)\n",
    "    # Increment document counter if the tag changes (indicating a new document)\n",
    "    if tag != prev_tag:\n",
    "        document_counter += 1 if prev_tag is not None else 0\n",
    "        chunk_counter = 0  # Reset chunk counter for a new document\n",
    "        prev_tag = tag  # Update the previous tag\n",
    "    # Create the metadata with chunk number and the PDF name from the tag\n",
    "    metadata = {'chunk': chunk_counter, 'pdf source': tag, 'source': map_pdf_to_web_dictionary[tag], 'text': text}\n",
    "    metadata_list.append(metadata)\n",
    "    # Increment the chunk counter\n",
    "    chunk_counter += 1\n",
    "# Create the ids based on the document and chunk counters\n",
    "ids = [f\"{document_counter}-{i}\" for i in range(len(embeddings))]\n",
    "\n",
    "# Create a DataFrame of the embeddings which will be used for our database later on\n",
    "hpInstallerEmbeddingsDF = pd.DataFrame({\n",
    "    'id': ids,  # Use the ids generated\n",
    "    'values': embeddings, # The vector embeddings which have been generated\n",
    "    'metadata': metadata_list # All the necessary metadata\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c86a1ed-23d4-4374-83b6-7f79758340f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print head of dataframe\n",
    "hpInstallerEmbeddingsDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d8122-43e4-4c23-bb5c-e61951bf467e",
   "metadata": {},
   "source": [
    "Next we initialize the vector database. A vector database is like a vast library where instead of books, you have complex ideas and information stored as numbers in a way that machines can quickly find, compare, and understand them. It's designed to handle and search through these numerical codes (vectors) efficiently, helping to provide fast and relevant results when you're looking for specific pieces of information.  We can create a free API key with Pinecone, then we create the index:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc59bb20",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc4e965-d32c-46d0-97b9-6f7be9251cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "import time\n",
    "\n",
    "# Index name for the heat pump chatbot\n",
    "hp_chatbot_index_name = 'chatbot-onboarding'\n",
    "PINECONE_API_KEY = os.getenv('<PINECONE API KEY>') or '<PINECONE API KEY>'\n",
    "PINECONE_ENVIRONMENT = os.getenv('gcp-starter') or 'gcp-starter'\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_ENVIRONMENT\n",
    ")\n",
    "\n",
    "# Create a new Pinecone index if it doesn't exist\n",
    "if hp_chatbot_index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        name=index_name,\n",
    "        metric='cosine',\n",
    "        dimension=1536,  # 1536 dim of text-embedding-ada-002\n",
    "        metadata_config={'indexed': ['chunk', 'source']}\n",
    "    )\n",
    "    time.sleep(1)\n",
    "# Initialize the Pinecone index\n",
    "hp_chatbot_index = pinecone.GRPCIndex(hp_chatbot_index_name)\n",
    "# Upsert data from DataFrame to the Pinecone index\n",
    "hp_chatbot_index.upsert_from_dataframe(hpInstallerEmbeddingsDF, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aef5d3e-d851-4841-92ca-e214d3a1f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Initialize OpenAI embeddings\n",
    "OPENAI_API_KEY = os.getenv('<INSERT OPEN API KEY>') or '<INSERT OPEN API KEY>'\n",
    "embed = OpenAIEmbeddings(\n",
    "    model='text-embedding-ada-002',\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "# Set up Pinecone vector store\n",
    "hp_chatbot_index = pinecone.Index(hp_chatbot_index_name)\n",
    "vectorstore = Pinecone(hp_chatbot_index, embed.embed_query, \"text\")\n",
    "\n",
    "# Set up the chatbot language model\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# Create a QA chain with retrieval from vectorstore\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# Example question\n",
    "question = \"Can you tell me how to deal with condensation run off for the NIBE F2040 heat pump?\"\n",
    "answer = qa(question)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asf",
   "language": "python",
   "name": "asf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
